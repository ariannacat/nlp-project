{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrgZL-aVvPIA"
      },
      "source": [
        "## Evaluation Framework\n",
        "\n",
        "We designed a custom evaluation pipeline to assess our generative models on multiple quality dimensions, combining semantic, stylistic, and structural criteria. The evaluation runs on four core queries and reports aggregate metrics across all responses. Here's how it works:\n",
        "\n",
        "### 1. Corpus Setup + Embedding Style Vector  \n",
        "We first load our cleaned Einstein corpus and randomly select up to 200 passages. Using the `all-MiniLM-L6-v2` encoder, we embed each passage and compute the average embedding across the sample. This gives us a reference “style vector” representing the general tone and semantics of Einstein’s writing. Later, we use this vector to compute how stylistically aligned a generated answer is.\n",
        "\n",
        "### 2. Query-Based Evaluation  \n",
        "We define four standard queries that probe conceptual and scientific themes in Einstein’s thought. For each query, we generate a response using the provided model function and record various linguistic and semantic properties.\n",
        "\n",
        "### 3. Style & Relevance Metrics  \n",
        "- **StyleSim**: Cosine similarity between the answer’s embedding and the precomputed Einstein style vector. High values indicate stylistic alignment with the corpus.  \n",
        "- **QuerySim**: Cosine similarity between the query embedding and the response embedding, measuring how semantically relevant the answer is.  \n",
        "- **Repetition**: Using TF-IDF cosine similarity, we compare all sentence pairs in the answer. High values mean redundant phrasing, which we penalize.\n",
        "\n",
        "### 4. Fluency & Diversity Metrics  \n",
        "- **Tokens**: Total number of words in the response  \n",
        "- **LexDiv**: Lexical diversity — unique words over total words  \n",
        "- **Distinct-1 / Distinct-2**: Measures how often unigrams and bigrams are repeated  \n",
        "- **AvgSentLen**: Average number of words per sentence — a proxy for sentence complexity\n",
        "\n",
        "### 5. Timing  \n",
        "- **GenTime(s)**: Time in seconds to generate each response — useful for comparing model speed\n",
        "\n",
        "The final output is a `pandas` DataFrame with per-query metrics and a summary of average values. This lets us compare different models not just by performance or plausibility, but also by elegance, precision, and stylistic authenticity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zNcSuo_P5Jl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "with open('einstein_300_questions.txt', 'r') as f:\n",
        "    queries = [line.strip().split('. ', 1)[-1] for line in f if line.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P_r3eO8D-qS",
        "outputId": "00780748-007e-4563-991a-420e4d7d9f58",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Load cleaned Einstein corpus and compute mean style embedding\n",
        "with open('einstein_cleaned_final.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = [l.strip() for l in f if l.strip()]\n",
        "\n",
        "# Initialize embedder and compute mean embedding of a random sample of passages\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "sample = np.random.choice(lines, min(len(lines), 200), replace=False)\n",
        "mean_emb = embedder.encode(sample.tolist(), convert_to_numpy=True).mean(axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKTyd6WrEAnG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Auxiliary function to compute internal repetition score\n",
        "def repetition_score(text):\n",
        "    sentences = [s.strip() for s in text.split('.') if len(s.strip().split()) > 3]\n",
        "    if len(sentences) < 2:\n",
        "        return 0.0\n",
        "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
        "    sim_matrix = cosine_similarity(tfidf)\n",
        "    n = len(sentences)\n",
        "    repetition = (sim_matrix.sum() - n) / (n * (n - 1))\n",
        "    return repetition\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate_model_metrics(model_fn, model_name=\"CustomModel\", verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate textual quality and semantic relevance of a generative model.\n",
        "\n",
        "    Args:\n",
        "        model_fn (function): A function that takes a query string and returns a generated response.\n",
        "        model_name (str): Optional name for the model (used in output).\n",
        "        verbose (bool): If True, prints each query and generated response.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of average evaluation metrics.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for query in queries:\n",
        "        start = time.time()\n",
        "        response = model_fn(query)\n",
        "        if response is None:\n",
        "            print(f\"[ERROR] No valid response for query: {query}\")\n",
        "            continue\n",
        "\n",
        "        gen_time = time.time() - start\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n[Query] {query}\\n[Response] {response}\\n\")\n",
        "\n",
        "        tokens = response.split()\n",
        "        length = len(tokens)\n",
        "        lex_div = len(set(tokens)) / length if length else 0\n",
        "        bigrams = list(zip(tokens, tokens[1:]))\n",
        "        distinct1 = len(set(tokens)) / length if length else 0\n",
        "        distinct2 = len(set(bigrams)) / len(bigrams) if bigrams else 0\n",
        "        sentences = [s.strip() for s in response.split('.') if s.strip()]\n",
        "        sent_lens = [len(s.split()) for s in sentences]\n",
        "        avg_sent_len = np.mean(sent_lens) if sent_lens else 0\n",
        "\n",
        "        # Style similarity to Einstein corpus\n",
        "        resp_emb = embedder.encode([response], convert_to_numpy=True)[0]\n",
        "        style_sim = float(np.dot(resp_emb, mean_emb) / (np.linalg.norm(resp_emb) * np.linalg.norm(mean_emb)))\n",
        "\n",
        "        # Semantic relevance (query-to-response similarity)\n",
        "        query_emb = embedder.encode([query], convert_to_numpy=True)[0]\n",
        "        query_sim = float(np.dot(query_emb, resp_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(resp_emb)))\n",
        "\n",
        "        # Internal repetition\n",
        "        repetition = repetition_score(response)\n",
        "\n",
        "        records.append({\n",
        "            \"Query\": query,\n",
        "            \"Tokens\": length,\n",
        "            \"LexDiv\": lex_div,\n",
        "            \"Distinct-1\": distinct1,\n",
        "            \"Distinct-2\": distinct2,\n",
        "            \"AvgSentLen\": avg_sent_len,\n",
        "            \"StyleSim\": style_sim,\n",
        "            \"QuerySim\": query_sim,\n",
        "            \"Repetition\": repetition,\n",
        "            \"GenTime(s)\": round(gen_time, 3)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    metric_cols = [\"Tokens\", \"LexDiv\", \"Distinct-1\", \"Distinct-2\", \"AvgSentLen\",\n",
        "                   \"StyleSim\", \"QuerySim\", \"Repetition\", \"GenTime(s)\"]\n",
        "    summary = df[metric_cols].mean().to_dict()\n",
        "\n",
        "    print(f\"\\n=== Metrics for {model_name} ===\")\n",
        "    for metric, value in summary.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkMdzbjcuNKS"
      },
      "source": [
        "The baseline is a generator which is trained just on its own parameters and not referring to files that, in other models, we retrieve, so the baseline answers out of its own pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRZ8xdLiHQCA",
        "outputId": "c0cb6ce0-e763-492c-b5a9-b0e46caa0c43",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8e63aa6638384e07b54d1e27d67414cb",
            "954804e3257147c4ba8471e9315c62a2",
            "3ff37ad43dd94937aafdfd82e5699573",
            "8b5646038f8a4b4695b2561ea51cb1fe",
            "f4bdc4260f5a4edca2253a470432691f",
            "dc0a4585af344019b1d8519b498c0778",
            "05190dcd038b49f588cf092c2bd30d37",
            "cce4d3bad1f8405d9e30a332c484eb42",
            "935539668da54175807551ee30631325",
            "96351a8aef8d4186a8568c82ea12a548",
            "aed914f6ddb9495cb8c7daaed7928f9d"
          ]
        },
        "id": "HcfAtXQGEKpf",
        "outputId": "194846ac-636a-43a1-93ee-2534da5234c9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from IPython.display import display\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import faiss\n",
        "\n",
        "# Load FAISS index\n",
        "print(\"[EVAL] Building FAISS index...\")\n",
        "passage_embeddings = embedder.encode(lines, convert_to_numpy=True, show_progress_bar=True)\n",
        "index = faiss.IndexFlatL2(passage_embeddings.shape[1])\n",
        "index.add(passage_embeddings)\n",
        "print(\"[EVAL] FAISS index ready.\")\n",
        "\n",
        "baseline_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "baseline_model     = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "baseline_pipe      = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=baseline_model,\n",
        "    tokenizer=baseline_tokenizer,\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "def baseline_chatbot(prompt: str, max_new_tokens: int = 50):\n",
        "    full = f\"You are a helpful assistant. Answer succinctly:\\n\\nQuestion: {prompt}\\nAnswer:\"\n",
        "    out = baseline_pipe(\n",
        "        full,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        pad_token_id=baseline_tokenizer.eos_token_id\n",
        "    )[0][\"generated_text\"]\n",
        "    return out.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "\n",
        "evaluate_model_metrics(baseline_chatbot, model_name=\"BASELINE\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLH-xJRzxd5q"
      },
      "source": [
        "# MODEL 1: RAG\n",
        "\n",
        "We’ve built a simple Retrieval-Augmented Generation (RAG) system in six stages to combine information lookup with language generation:\n",
        "\n",
        "1) **Loading the Cleaned Corpus**  \n",
        "We read our preprocessed Einstein paragraphs from disk into a list called `passages`, skipping empty lines.\n",
        "\n",
        "2) **Embedding with SentenceTransformer**  \n",
        "We initialize the `all-MiniLM-L6-v2` SentenceTransformer, which maps any sentence or paragraph into a 384-dimensional real vector. We compute embeddings in batches to avoid memory issues, and then stack them into a single NumPy array called `passage_embeddings`. We deliberately chose this specific model because it’s lightweight, fast, and surprisingly accurate even without a GPU—ideal for encoding thousands of passages quickly and comparing them semantically to new questions.\n",
        "\n",
        "3) **Indexing with FAISS**  \n",
        "We feed all paragraph embeddings into FAISS (`IndexFlatL2`), a library optimized for fast nearest-neighbor search. This lets us retrieve the most semantically similar chunks to a given question using simple Euclidean distance, in just milliseconds, even on CPU.\n",
        "\n",
        "4) **Preparing the Generator**  \n",
        "We use the base GPT-2 model (`gpt2`) as our text generator via HuggingFace’s `pipeline`. Even though GPT-2 isn’t instruction-tuned, it still generates coherent English and can synthesize retrieved content when guided with the right prompt.\n",
        "\n",
        "5) **Retrieval + Generation (`rag_answer`)**  \n",
        "Given a question, we embed the query, retrieve the top-k most relevant paragraphs, and concatenate them with an instruction prompt like “You are Albert Einstein. Paraphrase and synthesize the ideas…” followed by the user’s question. This full prompt is sent to GPT-2, which generates up to 200 new tokens as the answer. We also print out the retrieved chunks and their distances for debugging purposes.\n",
        "\n",
        "6) **Silent Version (`rag_answer_silent`)**  \n",
        "This alternative version hides all the print statements and returns only the cleaned answer string, extracted after the “Answer:” marker. It’s useful for automatic evaluation or frontend integration. We also added a small postprocessing step to remove anything trailing after a newline or unintended repeat prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EZJSW_HA9eI",
        "outputId": "088d2b50-4a2c-4d4d-b43b-64af655e9b3b"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mw3Uv-eaxiCK",
        "outputId": "e9bf8664-6b54-4787-9075-7e3960b4a15a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "\n",
        "corpus_path = \"einstein_cleaned_final.txt\"\n",
        "print(f\"[1/6] Loading corpus from {corpus_path}...\")\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1/6] Done. {len(passages)} passages loaded.\\n\")\n",
        "\n",
        "print(\"[2/6] Initializing SentenceTransformer embedder...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"[2/6] Embedder ready.\\n\")\n",
        "\n",
        "batch_size = 64\n",
        "all_embeddings = []\n",
        "print(\"[3/6] Computing embeddings in batches:\")\n",
        "start_time = time.time()\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i:i+batch_size]\n",
        "    embs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "    all_embeddings.append(embs)\n",
        "    print(f\"    • Batch {i//batch_size+1}/{(len(passages)-1)//batch_size+1} done\", flush=True)\n",
        "passage_embeddings = __import__('numpy').vstack(all_embeddings)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"[3/6] Embeddings computed: shape={passage_embeddings.shape}, time={elapsed:.1f}s\\n\")\n",
        "\n",
        "dim = passage_embeddings.shape[1]\n",
        "print(\"[4/6] Building FAISS index (IndexFlatL2)...\", flush=True)\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "start_time = time.time()\n",
        "index.add(passage_embeddings)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"[4/6] FAISS index built: {index.ntotal} vectors indexed in {elapsed:.2f}s\\n\", flush=True)\n",
        "\n",
        "print(\"[5/6] Initializing text-generation pipeline (GPT-2)...\", flush=True)\n",
        "generator = pipeline('text-generation', model='gpt2', tokenizer='gpt2')\n",
        "print(\"[5/6] Generator ready.\\n\", flush=True)\n",
        "\n",
        "def rag_answer(query, k=3):\n",
        "    print(f\"[6] Retrieving top {k} passages for query: {query!r}\", flush=True)\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    ctx = [passages[i] for i in I[0]]\n",
        "    for rank, (dist, text) in enumerate(zip(D[0], ctx), 1):\n",
        "        print(f\"    {rank}. (dist={dist:.3f}) {text[:60]}…\", flush=True)\n",
        "\n",
        "    prompt = (\n",
        "        \"You are Albert Einstein. Paraphrase and synthesize the ideas below in your own words, \"\n",
        "        \"avoiding verbatim quotes. Then answer:\\n\\n\"\n",
        "        + \"\\n---\\n\".join(ctx)\n",
        "        + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    out = generator(prompt, max_new_tokens=200, do_sample=True, early_stopping=True, top_p=0.9)[0]['generated_text']\n",
        "    print(\"[6] Generation complete.\\n\", flush=True)\n",
        "    return out\n",
        "\n",
        "def rag_answer_silent(query, k=3, max_new_tokens=200, top_p=0.8):\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    _, I = index.search(q_emb, k)\n",
        "    context = [passages[i] for i in I[0]]\n",
        "\n",
        "    prompt = (\n",
        "        \"You are Albert Einstein. Based on the following notes, answer the question simply and clearly:\\n\\n\"\n",
        "        + \"\\n---\\n\".join(context)\n",
        "        + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    generated_output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    # Extract only the part after 'Answer:' and stop at first break or repeated marker\n",
        "    answer = generated_output.split(\"Answer:\")[-1].strip()\n",
        "    for stop_token in [\"\\n\\n\", \"Question:\", \"Answer:\"]:\n",
        "        if stop_token in answer:\n",
        "            answer = answer.split(stop_token)[0].strip()\n",
        "    return answer\n",
        "\n",
        "# Example Usage\n",
        "'''\n",
        "for q in queries:\n",
        "    print(\"=== QUERY ===\")\n",
        "    print(q)\n",
        "    answer_verbose = rag_answer(q, k=3)\n",
        "    answer_silent = rag_answer_silent(q, k=3)\n",
        "    print(\"=== ANSWER (verbose) ===\")\n",
        "    print(answer_verbose)\n",
        "    print(\"=== ANSWER (silent) ===\")\n",
        "    print(answer_silent)\n",
        "    print(\"\\n\")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TGFjS-LARPy",
        "outputId": "01978f50-d7df-4a7e-be1a-d8cb2bd34c68"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(rag_answer_silent, model_name=\"RAG\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Rw5zY_GFiZ"
      },
      "source": [
        "# MODEL 2: GPT-2 Large RAG Pipeline\n",
        "\n",
        "This model builds upon our first RAG system, but introduces some important upgrades—most notably a stronger language model (GPT-2 Large) and a more refined prompting strategy for better, more focused responses.\n",
        "\n",
        "1) **Loading the Cleaned Corpus**  \n",
        "Just like before, we load our cleaned Einstein corpus into a list of passages. Each passage is a self-contained snippet of text.\n",
        "\n",
        "2) **Embedding with SentenceTransformer**  \n",
        "We continue using `all-MiniLM-L6-v2` to embed the corpus into 384-dimensional vectors. This SentenceTransformer strikes the best balance between speed and semantic quality, especially on CPU.\n",
        "\n",
        "3) **Indexing with FAISS**  \n",
        "Again, we rely on FAISS’s `IndexFlatL2` to efficiently retrieve the top-k most semantically similar passages to any given query, based on L2 distance.\n",
        "\n",
        "4) **Upgrading the Generator: GPT-2 → GPT-2 Large**  \n",
        "Here’s the first major change: we switch from regular `gpt2` to the much more powerful `gpt2-large`. GPT-2 Large has significantly more parameters (774M vs. 124M), which allows it to generate much more coherent and nuanced responses. To make use of the model effectively, we also run it on GPU (`device=0`).\n",
        "\n",
        "5) **Refined Prompting Strategy**  \n",
        "Instead of a general “paraphrase and synthesize” prompt, we now use a much more precise instruction: Einstein is imagined answering a curious student in a clear, concise, and complete way—limited to 2-3 sentences. The prompt discourages vague or meta-style outputs and explicitly asks for a self-contained, final answer. This helps constrain GPT-2 Large to generate focused replies.\n",
        "\n",
        "6) **Trimming the Output for Clarity**  \n",
        "We extract the generated text and limit the final answer to the first 2-3 complete sentences, which avoids overly long or rambling outputs. We also do light postprocessing to clean up formatting and trailing punctuation.\n",
        "\n",
        "**Key Differences from Model 1:**\n",
        "- **Stronger Generator:** GPT-2 Large replaces GPT-2 for better fluency and reasoning.\n",
        "- **Sharper Prompting:** More detailed, instructional prompt tailored to concise and educational answers.\n",
        "- **Postprocessing:** Automatic trimming to 2-3 sentences for consistency across outputs.\n",
        "- **GPU Usage:** The pipeline is explicitly set to run on GPU for faster generation.\n",
        "\n",
        "Overall, this model delivers noticeably sharper and more polished responses, making it a strong baseline for generating grounded, Einstein-style answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p5z5g0I1E5zD",
        "outputId": "80fa0c5a-3891-450c-8f8e-19b005df7e80"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the cleaned corpus\n",
        "corpus_path = 'einstein_cleaned_final.txt'\n",
        "print(f\"[1/6] Loading corpus from {corpus_path}...\")\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1/6] Done. {len(passages)} passages loaded.\\n\")\n",
        "\n",
        "# Step 2: Initialize the embedding model\n",
        "print(\"[2/6] Initializing SentenceTransformer embedder...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"[2/6] Embedder ready.\\n\")\n",
        "\n",
        "# Step 3: Compute and stack passage embeddings\n",
        "batch_size = 64\n",
        "all_embeddings = []\n",
        "print(\"[3/6] Computing embeddings in batches:\")\n",
        "start_time = time.time()\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i:i+batch_size]\n",
        "    embs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "    all_embeddings.append(embs)\n",
        "    print(f\"    • Batch {i//batch_size+1}/{(len(passages)-1)//batch_size+1} done\", flush=True)\n",
        "\n",
        "passage_embeddings = np.vstack(all_embeddings)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"[3/6] Embeddings computed: shape={passage_embeddings.shape}, time={elapsed:.1f}s\\n\")\n",
        "\n",
        "# Step 4: Build FAISS index for similarity search\n",
        "dim = passage_embeddings.shape[1]\n",
        "print(\"[4/6] Building FAISS index (IndexFlatL2)...\", flush=True)\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "start_time = time.time()\n",
        "index.add(passage_embeddings)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"[4/6] FAISS index built: {index.ntotal} vectors indexed in {elapsed:.2f}s\\n\", flush=True)\n",
        "\n",
        "# Step 5: Load GPT-2 Large as the text generator\n",
        "print(\"[5/6] Initializing text-generation pipeline (GPT-2 Large)...\", flush=True)\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2-large\",\n",
        "    tokenizer=\"gpt2-large\",\n",
        "    device=0\n",
        ")\n",
        "print(\"[5/6] Generator ready.\\n\", flush=True)\n",
        "\n",
        "# Step 6: Define the RAG-style answer generation function\n",
        "def gpt_2_answer(query, k=3):\n",
        "    print(f\"[6] Retrieving top {k} passages for query: {query!r}\", flush=True)\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    context = [passages[i][:300] for i in I[0]]\n",
        "\n",
        "    for rank, (dist, text) in enumerate(zip(D[0], context), 1):\n",
        "        print(f\"    {rank}. (dist={dist:.3f}) {text[:60]}…\", flush=True)\n",
        "\n",
        "    prompt = (\n",
        "        \"Albert Einstein is asked a scientific question by a curious student. \"\n",
        "        \"Using only the notes below, he gives a clear, concise, and complete answer in 2 to 3 sentences. \"\n",
        "        \"He avoids lists, vague generalities, and meta-comments. The answer is final and self-contained.\\n\\n\"\n",
        "        + \"\\n---\\n\".join(context)\n",
        "        + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=120,\n",
        "        temperature=0.45,\n",
        "        top_p=0.72,\n",
        "        repetition_penalty=1.5,\n",
        "        do_sample=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "        eos_token_id=generator.tokenizer.eos_token_id,\n",
        "        return_full_text=False\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    # Extract and truncate the output to the first 2–3 complete sentences\n",
        "    answer = output.strip()\n",
        "    sentence_split = answer.replace('\\n', ' ').split('. ')\n",
        "    if len(sentence_split) >= 3:\n",
        "        answer = '. '.join(sentence_split[:3]).strip()\n",
        "    elif len(sentence_split) >= 2:\n",
        "        answer = '. '.join(sentence_split[:2]).strip()\n",
        "    else:\n",
        "        answer = sentence_split[0].strip()\n",
        "\n",
        "    if not answer.endswith('.'):\n",
        "        answer += '.'\n",
        "\n",
        "    while answer[-1] in [':', ';']:\n",
        "        answer = answer[:-1].strip() + '.'\n",
        "\n",
        "    print(\"[6] Generation complete.\\n\")\n",
        "    return answer\n",
        "\n",
        "# Example usage:\n",
        "'''\n",
        "for q in queries:\n",
        "    print(\"=== QUERY ===\")\n",
        "    print(q)\n",
        "    answer = gpt_2_answer(q, k=3)\n",
        "    print(\"=== ANSWER ===\")\n",
        "    print(answer)\n",
        "    print(\"\\n\")\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7efAG7c-GJVF",
        "outputId": "33b9a5d4-8161-4b57-8147-ce1ec4f13dc9"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(gpt_2_answer, model_name=\"GPT2\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jruSA8MXHBMN"
      },
      "source": [
        "# MODEL 3: GPT-2 Large with Advanced Prompting\n",
        "\n",
        "In this third model, we build on the previous GPT-2 Large RAG system by introducing a more polished prompting strategy and better output control, aiming for answers that are not just relevant, but stylistically clear, final, and self-contained.\n",
        "\n",
        "1) **Loading the Cleaned Corpus**  \n",
        "As with previous models, we load our cleaned Einstein corpus into memory, storing each paragraph as a separate entry in a list called `passages`.\n",
        "\n",
        "2) **Embedding with SentenceTransformer**  \n",
        "We continue to use `all-MiniLM-L6-v2` for computing 384-dimensional semantic embeddings of each passage. The model is lightweight yet accurate enough to retrieve relevant chunks from our corpus efficiently, even without a GPU.\n",
        "\n",
        "3) **Indexing with FAISS**  \n",
        "We index all passage embeddings using FAISS (`IndexFlatL2`) to enable fast similarity search. This step remains unchanged from previous models.\n",
        "\n",
        "4) **Generator: GPT-2 Large**  \n",
        "We again use `gpt2-large` as the generator, running it on GPU for speed. The model's large size allows it to generate more coherent and meaningful responses than the base GPT-2.\n",
        "\n",
        "5) **Prompt Formatting Upgrade**  \n",
        "This is the core improvement of this model. We define a dedicated `format_prompt()` function to construct consistent, high-quality prompts. The new prompt emphasizes natural, self-contained language and instructs the model to avoid vague generalities or references to external information. The idea is to simulate how Einstein would respond thoughtfully and clearly to a curious question using only the retrieved notes.\n",
        "\n",
        "6) **Cleaner Answer Extraction**  \n",
        "We improve post-processing by cleaning the generated output to extract just the answer portion (after “Answer:”), trimming any trailing artifacts or prompt echoes. We also truncate the text after the last full sentence to ensure brevity and clarity.\n",
        "\n",
        "**Key Differences from Model 2:**\n",
        "- **Dedicated prompt formatter** to ensure consistent tone and structure across queries.\n",
        "- **Better answer cleanup**: more robust text extraction and trimming logic.\n",
        "- **Tighter sampling controls**: slightly lower temperature and higher repetition penalty for more focused responses.\n",
        "\n",
        "This model represents our most refined setup so far, blending strong retrieval, a powerful generator, and a well-crafted prompt to produce high-quality Einstein-style answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KWHOzhveGkLb",
        "outputId": "1dc06db1-697d-4007-bb13-9425f20a960c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 1: Load the preprocessed Einstein corpus\n",
        "corpus_path = 'einstein_cleaned_final.txt'\n",
        "print(f\"[1/6] Loading corpus from {corpus_path}...\")\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1/6] Done. {len(passages)} passages loaded.\\n\")\n",
        "\n",
        "# Step 2: Initialize the sentence embedder\n",
        "print(\"[2/6] Initializing SentenceTransformer embedder...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"[2/6] Embedder ready.\\n\")\n",
        "\n",
        "# Step 3: Compute and stack embeddings for all passages\n",
        "batch_size = 64\n",
        "all_embeddings = []\n",
        "print(\"[3/6] Computing embeddings in batches:\")\n",
        "start_time = time.time()\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i:i+batch_size]\n",
        "    embs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "    all_embeddings.append(embs)\n",
        "    print(f\"    • Batch {i//batch_size+1}/{(len(passages)-1)//batch_size+1} done\", flush=True)\n",
        "passage_embeddings = np.vstack(all_embeddings)\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"[3/6] Embeddings ready (shape={passage_embeddings.shape}) in {elapsed:.1f}s\\n\")\n",
        "\n",
        "# Step 4: Build a FAISS L2 similarity index for the passage embeddings\n",
        "dim = passage_embeddings.shape[1]\n",
        "print(\"[4/6] Building FAISS index...\")\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(passage_embeddings)\n",
        "print(f\"[4/6] Indexed {index.ntotal} vectors.\\n\")\n",
        "\n",
        "# Step 5: Initialize the text generation model (GPT-2 Large)\n",
        "print(\"[5/6] Loading generation pipeline...\")\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2-large\",\n",
        "    tokenizer=\"gpt2-large\",\n",
        "    device=0  # CUDA:0\n",
        ")\n",
        "print(\"[5/6] Generator ready.\\n\")\n",
        "\n",
        "# Format the generation prompt with retrieved context and structured instructions\n",
        "def format_prompt(query, context):\n",
        "    return (\n",
        "        \"You are Albert Einstein. Based on the following knowledge, answer the question thoughtfully and directly. \"\n",
        "        \"Use clear, natural language. Write 2 or 3 complete sentences that stand alone. Do not refer to sources or background information.\\n\\n\"\n",
        "        + \"\\n---\\n\".join(context)\n",
        "        + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "# Step 6: Define the RAG-based answer function using GPT-2 Large\n",
        "def rag_advanced_answer(query, k=3):\n",
        "    print(f\"[6] Retrieving top {k} passages for query: {query!r}\")\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    ctx = [passages[i][:300] for i in I[0]]\n",
        "\n",
        "    for rank, (dist, text) in enumerate(zip(D[0], ctx), 1):\n",
        "        print(f\"    {rank}. (dist={dist:.3f}) {text[:60]}…\")\n",
        "\n",
        "    prompt = format_prompt(query, ctx)\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=120,\n",
        "        temperature=0.4,\n",
        "        top_p=0.85,\n",
        "        repetition_penalty=1.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "        eos_token_id=generator.tokenizer.eos_token_id\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    # Extract the answer text following \"Answer:\" and clean termination\n",
        "    answer = output.split(\"Answer:\")[-1].strip()\n",
        "    for stop_token in [\"\\n\\n\", \"\\nQuestion:\", \"\\nAnswer:\", \"---\"]:\n",
        "        if stop_token in answer:\n",
        "            answer = answer.split(stop_token)[0].strip()\n",
        "    if '.' in answer:\n",
        "        answer = '.'.join(answer.split('.')[:-1]) + '.'\n",
        "    print(\"[6] Generation complete.\\n\")\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "'''\n",
        "for q in queries:\n",
        "    print(\"=== QUERY ===\")\n",
        "    print(q)\n",
        "    answer = rag_advanced_answer(q, k=3)\n",
        "    print(\"=== ANSWER ===\")\n",
        "    print(answer)\n",
        "    print(\"\\n\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxI3NcPoHEYP",
        "outputId": "97ef3b66-42f6-419a-d65c-e5abe5915adf"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(rag_advanced_answer, model_name=\"RAG advanced\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6wxfvTgH7M9"
      },
      "source": [
        "# MODEL 4: GPT-Neo 2.7B with RAG\n",
        "\n",
        "In this fourth model, we scale up significantly by replacing GPT-2 with GPT-Neo 2.7B—a powerful open-source language model developed by EleutherAI that brings a big jump in fluency and reasoning capacity. We also increase the number of retrieved passages and optimize the prompt-to-token flow for better long-form generation.\n",
        "\n",
        "1) **Loading the Corpus**  \n",
        "As before, we load our pre-cleaned corpus of Einstein passages from disk and store them as a list of paragraphs.\n",
        "\n",
        "2) **Embedding with SentenceTransformer**  \n",
        "We keep using `all-MiniLM-L6-v2` to embed each paragraph into 384-dimensional vectors. It's fast and effective for semantic similarity and works well with our retrieval setup.\n",
        "\n",
        "3) **Indexing with FAISS**  \n",
        "All embeddings are indexed using FAISS (`IndexFlatL2`), enabling fast L2-based nearest-neighbor search. Nothing changes here compared to earlier models.\n",
        "\n",
        "4) **Loading GPT-Neo (2.7B)**  \n",
        "Here’s the major upgrade: we move from GPT-2 Large (774M) to GPT-Neo 2.7B. This model has over 3 times more parameters and was trained on The Pile, making it significantly more capable for general knowledge reasoning and natural language generation. We load both the tokenizer and model using HuggingFace, and run them on GPU if available.\n",
        "\n",
        "5) **Creating a Custom Generation Pipeline**  \n",
        "We manually create the generation pipeline by loading the model and tokenizer, and ensure that both are sent to the correct device (GPU if present).\n",
        "\n",
        "6) **RAG Answer Function (`rag_gptneo`)**  \n",
        "We increase `k` to 7, retrieving more relevant context chunks per query. These are concatenated with section markers (`---`) and followed by a minimalistic prompt:  \n",
        "`Question: ... Answer:`  \n",
        "Unlike earlier models, there is no \"Einstein persona\" or stylistic guidance—GPT-Neo is expected to synthesize the context into a coherent answer on its own.\n",
        "\n",
        "7) **Token Management and Truncation**  \n",
        "We calculate the token length of the input prompt and truncate it if it exceeds the model’s token limit. We manually send the `input_ids` tensor to GPU to ensure the generation runs efficiently.\n",
        "\n",
        "8) **Answer Extraction**  \n",
        "We split the final output at the “Answer:” tag (if present), and return the cleaned answer as plain text.\n",
        "\n",
        "**Key Differences from Model 3:**\n",
        "- **Much larger model:** GPT-Neo 2.7B instead of GPT-2 Large → higher fluency and reasoning power.\n",
        "- **Manual token flow:** Explicit GPU handling and input truncation using tokenizer limits.\n",
        "- **No persona prompt:** More neutral style, letting the model generate based solely on retrieved knowledge.\n",
        "- **Higher context depth:** `k=7` passages retrieved instead of 3 for a richer information base.\n",
        "\n",
        "This model pushes the limits of our RAG setup in terms of scale, leveraging a high-capacity generator and feeding it more supporting context to produce deeper, more informative answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3oHO0tEUHOsF",
        "outputId": "fe7c8908-5115-4f9d-fc86-fd776c5e308b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the preprocessed Einstein corpus\n",
        "corpus_path = 'einstein_cleaned_final.txt'\n",
        "print(f\"[1/8] Loading corpus from {corpus_path}...\")\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1/8] Corpus loaded: {len(passages)} passages\\n\")\n",
        "\n",
        "# Step 2: Initialize the SentenceTransformer embedder\n",
        "print(\"[2/8] Initializing SentenceTransformer...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"[2/8] Embedder ready\\n\")\n",
        "\n",
        "# Step 3: Compute and index passage embeddings\n",
        "print(\"[3/8] Computing embeddings and building FAISS index...\")\n",
        "batch_size = 64\n",
        "emb_chunks = []\n",
        "start = time.time()\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i:i+batch_size]\n",
        "    embs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "    emb_chunks.append(embs)\n",
        "    print(f\"    [3/8] Batch {i//batch_size+1}/{(len(passages)-1)//batch_size+1} completed\")\n",
        "passage_embeddings = np.vstack(emb_chunks)\n",
        "dim = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(passage_embeddings)\n",
        "print(f\"[3/8] Embeddings indexed (shape={passage_embeddings.shape}) in {time.time()-start:.1f}s\\n\")\n",
        "\n",
        "# Step 4: Load the GPT-Neo 2.7B model and tokenizer\n",
        "print(\"[4/8] Initializing tokenizer and GPT-Neo model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"[4/8] Model and tokenizer loaded (device={'cuda' if device==0 else 'cpu'})\\n\")\n",
        "\n",
        "# Step 5: Create the text-generation pipeline\n",
        "print(\"[5/8] Building generation pipeline...\")\n",
        "generator = pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=device)\n",
        "print(\"[5/8] Pipeline ready\\n\")\n",
        "\n",
        "# Step 6–8: Define the full RAG inference process with GPT-Neo\n",
        "def rag_gptneo(query, k=7, max_new_tokens=300, top_p=0.9):\n",
        "    print(f\"\\n[6/8] Received query: {query!r}\")\n",
        "\n",
        "    # Step 6: Retrieve top-k semantically similar passages\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    context = [passages[i] for i in I[0]]\n",
        "\n",
        "    print(f\"[6/8] Retrieved top-{k} passages:\")\n",
        "    for rank, (dist, text) in enumerate(zip(D[0], context), 1):\n",
        "        print(f\"    {rank}. (dist={dist:.3f}) {text[:60]}…\")\n",
        "\n",
        "    # Step 7: Prepare the prompt for generation\n",
        "    context_text = \"\\n---\\n\".join(context)\n",
        "    prompt = f\"{context_text}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    # Ensure input fits within model's max input size\n",
        "    max_input_tokens = tokenizer.model_max_length\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
        "    input_ids = input_ids.to(generator_model.device)\n",
        "    print(f\"[7/8] Prompt ready (tokens={input_ids.shape[1]}/{max_input_tokens}, device={input_ids.device})\")\n",
        "\n",
        "    # Step 8: Generate the response\n",
        "    output_ids = generator_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the response portion following \"Answer:\"\n",
        "    if \"Answer:\" in generated_text:\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = generated_text.strip()\n",
        "\n",
        "    print(\"[8/8] Answer generated\")\n",
        "    return answer\n",
        "\n",
        "# Example Usage\n",
        "'''\n",
        "for q in example_queries:\n",
        "    print(f\"\\n=============================\\nQuery: {q}\")\n",
        "    answer = rag_gptneo(q)\n",
        "    print(f\"\\nAnswer:\\n{answer}\")\n",
        "    print(\"=============================\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U4DScEvH92F",
        "outputId": "6528515e-967e-4503-f620-9538c414bb6d"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(rag_gptneo, model_name=\"GPT-NEO\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE4gnzmEIipC"
      },
      "source": [
        "# MODEL 5: GPT-Neo 1.3B with Dynamic Context Truncation\n",
        "\n",
        "In this fifth model, we introduce a refined version of the GPT-Neo-based RAG pipeline. Instead of simply scaling the generator up, this version prioritizes *efficient use of the input token window* by dynamically adapting how much context to include for each query. We also adopt a more detailed, structured prompt to guide answer style and length.\n",
        "\n",
        "1) **Corpus and Embedding (unchanged)**  \n",
        "We reuse the same cleaned Einstein corpus and embed each paragraph using the `all-MiniLM-L6-v2` SentenceTransformer model. All embeddings are indexed using FAISS for fast nearest-neighbor search via L2 distance.\n",
        "\n",
        "2) **Generator Upgrade: GPT-Neo 1.3B**  \n",
        "We switch from GPT-Neo 2.7B to the lighter `gpt-neo-1.3B` model. While smaller, this version still provides high-quality outputs and is more manageable in environments with limited GPU memory (e.g., Colab).\n",
        "\n",
        "3) **Smart Context Assembly**  \n",
        "Instead of blindly retrieving `k=7` passages and concatenating all of them, we build the input **dynamically**, one paragraph at a time. After each addition, we test whether the resulting prompt (including the final question and instructions) still fits within the model’s token limit. This ensures we use *as much relevant context as possible* without truncation, which is especially important for long-generation models.\n",
        "\n",
        "4) **Prompting Strategy**  \n",
        "The prompt tells the model to act as Albert Einstein and provide a clear, thoughtful, and concise answer based on the retrieved \"excerpts.\" It explicitly instructs the model *not to mention the excerpts*, which helps avoid meta comments. The target answer length is 3–5 sentences.\n",
        "\n",
        "5) **Generation Details**  \n",
        "We sample up to 400 new tokens using `top_p` sampling with moderate temperature, and apply a `no_repeat_ngram_size` of 3 to prevent redundant phrasing.\n",
        "\n",
        "6) **Post-processing**  \n",
        "We extract the answer from the generated output after the “Answer:” token, clean it up, and trim trailing content after known markers (like “---” or repeated prompt sections). We also ensure the answer ends with a full stop and makes grammatical sense.\n",
        "\n",
        "**Key Differences from Model 4:**\n",
        "- **Smaller model (1.3B vs 2.7B)**, better for memory-constrained environments.\n",
        "- **Dynamic context selection** ensures optimal use of token budget with no waste or hard truncation.\n",
        "- **Stronger prompt structure** guides both tone and content.\n",
        "- **Improved repetition control** using `no_repeat_ngram_size`.\n",
        "\n",
        "This model finds a sweet spot between resource efficiency and generation quality. Thanks to smarter context management and focused prompting, it often produces answers that are just as good—if not better—than its larger predecessor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECXyv7SWKC3M"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WRX1oSKFKi5x",
        "outputId": "d92ce9d7-3503-4612-a0dd-b560afbff8e0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the cleaned corpus\n",
        "print(\"[1/8] Loading corpus...\")\n",
        "corpus_path = 'einstein_cleaned_final.txt'\n",
        "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1/8] Corpus loaded: {len(passages)} passages\\n\")\n",
        "\n",
        "# Step 2: Initialize the SentenceTransformer embedder\n",
        "print(\"[2/8] Initializing SentenceTransformer...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"[2/8] Embedder ready\\n\")\n",
        "\n",
        "# Step 3: Compute passage embeddings and build FAISS index\n",
        "print(\"[3/8] Computing embeddings and indexing...\")\n",
        "batch_size = 64\n",
        "emb_chunks = []\n",
        "start = time.time()\n",
        "for i in range(0, len(passages), batch_size):\n",
        "    batch = passages[i:i+batch_size]\n",
        "    embs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
        "    emb_chunks.append(embs)\n",
        "    print(f\"    [3/8] Batch {i//batch_size+1}/{(len(passages)-1)//batch_size+1} completed\")\n",
        "passage_embeddings = np.vstack(emb_chunks)\n",
        "dim = passage_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(passage_embeddings)\n",
        "print(f\"[3/8] Indexed {passage_embeddings.shape[0]} vectors in {time.time()-start:.1f}s\\n\")\n",
        "\n",
        "# Step 4: Load the GPT-Neo 1.3B model and tokenizer\n",
        "print(\"[4/8] Initializing tokenizer and GPT-Neo (1.3B)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "generator_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator_model.to(device)\n",
        "print(f\"[4/8] Model and tokenizer ready (device={device})\\n\")\n",
        "\n",
        "# Step 5–7: Define the RAG pipeline using GPT-Neo 1.3B\n",
        "def rag_gptneo_2(query, k=7, max_new_tokens=400, top_p=0.8, temperature=0.7):\n",
        "    print(f\"\\n[5/8] Processing query: {query!r}\")\n",
        "\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k)\n",
        "\n",
        "    total_tokens = 0\n",
        "    context_chunks = []\n",
        "\n",
        "    # Dynamically accumulate context passages until input length limit\n",
        "    for idx in I[0]:\n",
        "        para_text = passages[idx][:1500]\n",
        "        test_prompt = (\n",
        "            f\"You are Albert Einstein. Based on the following excerpts, answer the question in your own words. \"\n",
        "            \"Provide a clear, thoughtful, and concise answer in 3–5 complete sentences. \"\n",
        "            \"Do not refer to the excerpts or sources. Just answer directly.\\n\\n\"\n",
        "            \"Excerpts:\\n\" + \"\\n---\\n\".join(context_chunks + [para_text]) + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "        )\n",
        "        token_count = len(tokenizer.encode(test_prompt))\n",
        "\n",
        "        if token_count < tokenizer.model_max_length - max_new_tokens:\n",
        "            context_chunks.append(para_text)\n",
        "            total_tokens = token_count\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"[5/8] Collected {len(context_chunks)} context passages (approx. {total_tokens} tokens)\\n\")\n",
        "\n",
        "    context_text = \"\\n---\\n\".join(context_chunks)\n",
        "    prompt = (\n",
        "        f\"You are Albert Einstein. Based on the following excerpts, answer the question in your own words. \"\n",
        "        \"Provide a clear, thoughtful, and concise answer in 3–5 complete sentences. \"\n",
        "        \"Do not refer to the excerpts or sources. Just answer directly.\\n\\n\"\n",
        "        \"Excerpts:\\n\" + context_text + f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    max_input_tokens = tokenizer.model_max_length\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(device)\n",
        "    print(f\"[6/8] Prompt ready (tokens={input_ids.shape[1]}/{max_input_tokens})\")\n",
        "\n",
        "    output_ids = generator_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer portion\n",
        "    if \"Answer:\" in generated_text:\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = generated_text.strip()\n",
        "\n",
        "    for stop_token in [\"---\", \"\\n\\n\", \"\\nAnswer:\"]:\n",
        "        if stop_token in answer:\n",
        "            answer = answer.split(stop_token)[0].strip()\n",
        "    if '.' in answer:\n",
        "        answer = '.'.join(answer.split('.')[:-1]) + '.'\n",
        "\n",
        "    print(\"[7/8] Answer generated\")\n",
        "    return answer\n",
        "\n",
        "# Example Usage\n",
        "'''\n",
        "for q in example_queries:\n",
        "    print(f\"\\n=============================\\nQuery: {q}\")\n",
        "    answer = rag_gptneo_2(q)\n",
        "    print(f\"\\nAnswer:\\n{answer}\")\n",
        "    print(\"=============================\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVrgYsZaI9jd",
        "outputId": "cbde1eb6-b676-44f8-d19f-14e65c829d09"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(rag_gptneo_2, model_name=\"GPT-NEO 2\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YDVJc3JY-r"
      },
      "source": [
        "# MODEL 6: Flan-T5 Small with Cosine Nearest Neighbors\n",
        "\n",
        "In this sixth model, we shift from autoregressive generators like GPT-2 and GPT-Neo to a **sequence-to-sequence model**, specifically `Flan-T5 Small` by Google. This model is instruction-tuned out of the box, making it especially well-suited for prompt-based tasks like question answering and paraphrasing. We also replace FAISS with a simpler cosine similarity index using `scikit-learn`.\n",
        "\n",
        "1) **Loading the Corpus**  \n",
        "We load the cleaned corpus of Einstein passages into a list, as in all previous models.\n",
        "\n",
        "2) **Embedding the Passages**  \n",
        "We use the same lightweight SentenceTransformer (`all-MiniLM-L6-v2`) to embed each passage into a 384-dimensional vector. These embeddings serve as the basis for semantic retrieval.\n",
        "\n",
        "3) **Retrieval with Cosine Similarity**  \n",
        "Instead of using FAISS, we use `scikit-learn`’s `NearestNeighbors` with `metric='cosine'`. This is easier to set up and interpretable, and works well when dealing with a few thousand vectors. We retrieve the top `k=5` passages closest to the query embedding.\n",
        "\n",
        "4) **Switching to Flan-T5**  \n",
        "We use `google/flan-t5-small`, a small but instruction-finetuned T5 model that’s capable of following natural language prompts. Unlike GPT-based models, Flan-T5 takes an input string and generates an output string in a fully encoder-decoder architecture.\n",
        "\n",
        "5) **Prompt Design**  \n",
        "We use a new prompt format tailored to T5-style models:  \n",
        "“You are Albert Einstein. Based on the following excerpts, answer the question in first person, using your own words.”  \n",
        "The retrieved context is appended under “Excerpts:”, followed by the question. This style plays to Flan-T5’s strengths—concise, factual, instruction-following answers.\n",
        "\n",
        "6) **Controlled Generation**  \n",
        "We use nucleus sampling (`top_p=0.9`) with moderate temperature (`0.7`) to allow creativity while maintaining relevance. The `min_length` and `max_length` parameters help ensure that the answer is reasonably detailed and doesn’t terminate too early.\n",
        "\n",
        "7) **Answer Extraction**  \n",
        "The generated output is split on “Answer:” to isolate the response cleanly, which is then printed and returned.\n",
        "\n",
        "**Key Differences from Previous Models:**\n",
        "- **Different model architecture:** Flan-T5 is encoder-decoder, not autoregressive.\n",
        "- **Instruction tuning:** Flan-T5 was pretrained with supervised tasks and follows prompts better out of the box.\n",
        "- **Simplified retriever:** NearestNeighbors with cosine distance replaces FAISS.\n",
        "- **Different output style:** More focused, structured, and generally shorter answers, often with a clearer logical flow.\n",
        "\n",
        "This model trades raw generative power for interpretability and prompt-following ability, making it ideal for concise, structured Einstein-like answers grounded in retrieved content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152,
          "referenced_widgets": [
            "a70a686cdc594b8b8254d7d71bc0a048",
            "68bd0685c97d47c9bf01a1759098e627",
            "1ad6234838194cccb3678c46aa51fddb",
            "3213baef2b1a47fa9e02b7bfc9fbeb55",
            "a77e27b6543d4f2094640f3547705202",
            "24817f18a2564c6aa172a7c7b255ebe7",
            "394a77c758af484e938a8d73a3ccfc89",
            "d6cd8e41a13c42fcb3e4e57b71f94584",
            "bc76cec45ace4855b965c5e9cad67527",
            "c5b82406fa0b456690e7c25bc12d0119",
            "4d3f29d49190423780eb18f593fd2cc7"
          ]
        },
        "id": "o7biOAU0JHis",
        "outputId": "78f095fa-65eb-45f9-d37e-20130e2ceb47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Step 1: Load the cleaned corpus\n",
        "CLEANED_PATH = 'einstein_cleaned_final.txt'\n",
        "EMBED_MODEL = 'all-MiniLM-L6-v2'\n",
        "GEN_MODEL = 'google/flan-t5-small'\n",
        "K = 5\n",
        "\n",
        "with open(CLEANED_PATH, 'r', encoding='utf-8') as f:\n",
        "    passages = [line.strip() for line in f if line.strip()]\n",
        "print(f\"[1] Loaded {len(passages)} passages.\")\n",
        "\n",
        "# Step 2: Embed the passages using a SentenceTransformer\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "print(\"[2] Computing embeddings...\")\n",
        "embeddings = embedder.encode(passages, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Step 3: Build a Nearest Neighbors index using cosine similarity\n",
        "nn = NearestNeighbors(n_neighbors=K, metric='cosine')\n",
        "nn.fit(embeddings)\n",
        "print(\"[3] NearestNeighbors index ready.\")\n",
        "\n",
        "# Step 4: Load Flan-T5 model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)\n",
        "generator = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1,  # CPU (set to 0 for CUDA if available)\n",
        ")\n",
        "\n",
        "# Step 5: Define the RAG answer generation function\n",
        "def rag_flan_t5(query, k=K, max_length=500, min_length=150, top_p=0.9, temperature=0.7):\n",
        "    t0 = time.time()\n",
        "    print(f\"\\n[RAG-FLAN] Query: {query}\")\n",
        "\n",
        "    # Retrieve top-k relevant passages based on cosine similarity\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    dists, idxs = nn.kneighbors(q_emb, n_neighbors=k)\n",
        "    retrieved = [passages[i] for i in idxs[0]]\n",
        "    print(f\"[RAG-FLAN] Retrieved passages (cosine distances): {dists[0].round(3).tolist()}\")\n",
        "\n",
        "    # Compose the prompt for conditional generation\n",
        "    context_text = \"\\n---\\n\".join(retrieved)\n",
        "    prompt = (\n",
        "        \"You are Albert Einstein. Based on the following excerpts, answer the question in first person, using your own words.\\n\\n\"\n",
        "        \"Excerpts:\\n\" + context_text +\n",
        "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate the answer using the T5 model\n",
        "    out = generator(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    # Extract and clean the answer\n",
        "    answer = out.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    print(\"\\n=== EINSTEIN'S ANSWER ===\")\n",
        "    print(answer)\n",
        "    print(f\"[RAG-FLAN] Done in {time.time() - t0:.2f}s\\n\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "'''# Step 6: Run the model if executed as main\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"What is the nature of time?\"\n",
        "    rag_flan_t5(query)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQMxjakWJs65",
        "outputId": "a5eca3c2-ec90-4261-9e7f-563919e4816a"
      },
      "outputs": [],
      "source": [
        "evaluate_model_metrics(rag_flan_t5, model_name=\"FLAN 5\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4dWaYQeMOFP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05190dcd038b49f588cf092c2bd30d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ad6234838194cccb3678c46aa51fddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6cd8e41a13c42fcb3e4e57b71f94584",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc76cec45ace4855b965c5e9cad67527",
            "value": 101
          }
        },
        "24817f18a2564c6aa172a7c7b255ebe7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3213baef2b1a47fa9e02b7bfc9fbeb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b82406fa0b456690e7c25bc12d0119",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3f29d49190423780eb18f593fd2cc7",
            "value": " 101/101 [00:07&lt;00:00, 16.63it/s]"
          }
        },
        "394a77c758af484e938a8d73a3ccfc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ff37ad43dd94937aafdfd82e5699573": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cce4d3bad1f8405d9e30a332c484eb42",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_935539668da54175807551ee30631325",
            "value": 101
          }
        },
        "4d3f29d49190423780eb18f593fd2cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68bd0685c97d47c9bf01a1759098e627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24817f18a2564c6aa172a7c7b255ebe7",
            "placeholder": "​",
            "style": "IPY_MODEL_394a77c758af484e938a8d73a3ccfc89",
            "value": "Batches: 100%"
          }
        },
        "8b5646038f8a4b4695b2561ea51cb1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96351a8aef8d4186a8568c82ea12a548",
            "placeholder": "​",
            "style": "IPY_MODEL_aed914f6ddb9495cb8c7daaed7928f9d",
            "value": " 101/101 [00:09&lt;00:00, 12.10it/s]"
          }
        },
        "8e63aa6638384e07b54d1e27d67414cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_954804e3257147c4ba8471e9315c62a2",
              "IPY_MODEL_3ff37ad43dd94937aafdfd82e5699573",
              "IPY_MODEL_8b5646038f8a4b4695b2561ea51cb1fe"
            ],
            "layout": "IPY_MODEL_f4bdc4260f5a4edca2253a470432691f"
          }
        },
        "935539668da54175807551ee30631325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "954804e3257147c4ba8471e9315c62a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0a4585af344019b1d8519b498c0778",
            "placeholder": "​",
            "style": "IPY_MODEL_05190dcd038b49f588cf092c2bd30d37",
            "value": "Batches: 100%"
          }
        },
        "96351a8aef8d4186a8568c82ea12a548": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70a686cdc594b8b8254d7d71bc0a048": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68bd0685c97d47c9bf01a1759098e627",
              "IPY_MODEL_1ad6234838194cccb3678c46aa51fddb",
              "IPY_MODEL_3213baef2b1a47fa9e02b7bfc9fbeb55"
            ],
            "layout": "IPY_MODEL_a77e27b6543d4f2094640f3547705202"
          }
        },
        "a77e27b6543d4f2094640f3547705202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed914f6ddb9495cb8c7daaed7928f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc76cec45ace4855b965c5e9cad67527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5b82406fa0b456690e7c25bc12d0119": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce4d3bad1f8405d9e30a332c484eb42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6cd8e41a13c42fcb3e4e57b71f94584": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0a4585af344019b1d8519b498c0778": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bdc4260f5a4edca2253a470432691f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
